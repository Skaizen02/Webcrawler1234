#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
links <- c(url)
crawler <- function(iterations, url, path) {
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (i in 0:length(links)) {
View(links)
link <- links[i]
View(link)
if (!(link %in% scannedLinks)) {
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
links <- c(url)
crawler <- function(iterations, url, path) {
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (i in 0:length(links)) {
View(links)
link <- links[i+1]
View(link)
if (!(link %in% scannedLinks)) {
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
links[i]
links[1]
source('~/GitHub/Webcrawler1/WebCrawler.R')
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
links <- c(url)
crawler <- function(iterations, url, path) {
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (i in 0:length(links)) {
View(links)
link <- links[i+1]
View(link)
if (!(link %in% scannedLinks)) {
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
source('~/GitHub/Webcrawler1/WebCrawler.R')
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
View(link)
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
links <- c(url)
crawler <- function(iterations, url, path) {
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (i in 0:length(links)) {
link <- links[i+1]
if (!(link %in% scannedLinks)) {
View(link)
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
debugSource('~/GitHub/Webcrawler1/WebCrawler.R')
source('~/GitHub/Webcrawler1/WebCrawler.R')
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
View(link)
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
links <- c(url)
crawler <- function(iterations, url, path) {
scannedLinks <- c()
for (i in 0:iterations) {
tmp <- links
for (j in 0:length(links)) {
link <- links[j+1]
if (!(link %in% scannedLinks)) {
View(link)
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
debugSource('~/GitHub/Webcrawler1/WebCrawler.R')
source('~/GitHub/Webcrawler1/WebCrawler.R')
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
length(links)
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
df <- crawler(iterations = 2, url = "https://www.finextra.com/", path = "https://www.finextra.com/newsarticle/*")
debugSource('~/GitHub/Webcrawler1/WebCrawler.R')
source('~/GitHub/Webcrawler1/WebCrawler.R')
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
crawler <- function(iterations, url, path) {
scannedLinks <- c()
links <- c(url)
for (i in 0:iterations) {
tmp <- links
for (j in seq.int(length(links))) {
link <- links[j+1]
if (!(link %in% scannedLinks)) {
View(link)
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
source('~/GitHub/Webcrawler1/WebCrawler.R')
library(Rcrawler)
library(rvest)
url <- "https://www.finextra.com/"
#Add titles to the database
#addTitles()
addTitles <- function() {
df <- data.frame(matrix(unlist(DATA)))
df <- na.omit(df)
names(df)[1] = "titles"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Add tags to the data base
#addTags()
addTags <- function() {
df <- data.frame(matrix(unlist(DATA)))
names(df)[1] = "tags"
df <- data.frame(titles = unique(df$titles))
db <- dbReadTable(con, "webcrawler")
df$tags <- list(NULL)
df$tags[0:length(db$tags)] <- db$tags
df$id <- seq.int(length(df$tags))
dbWriteTable(conn=con, name="webcrawler", value=df, row.names = FALSE, overwrite = TRUE)
}
#Find links
# links <- c()
# findLinks(url, 2)
#Crawler
titleSelector <- ".left.fullWidth:not(.left.fullWidth.upper.fontColorOne)"
tagSelector <- ".ncMetaDataSnippet"
crawler <- function(iterations, url, path) {
scannedLinks <- c()
links <- c(url)
for (i in 0:iterations) {
tmp <- links
for (j in seq.int(length(links))) {
link <- links[j+1]
if (!(link %in% scannedLinks)) {
View(link)
pageLinks <- LinkExtractor(link)[[2]]
tmp <- c(tmp, pageLinks)
scannedLinks <- c(scannedLinks, link)
}
}
links <- tmp
}
l <- length(links)
View(links)
# df <- data.frame(var1=character(l), var2=character(l), var3=character(1))
# df$urls <- links
# df$titles <- list(NULL)
# df$tags <- list(NULL)
# lapply(links, function(link) {
#   if (grepl(link, path)) {
#     html <- read_html(link)
#     title <- html_node(html, titleSelector)
#     tags <- html_nodes(html, tagSelector)
#     df$titles[url = link] <- title
#     df$tags[url = link] <- tags
#   }
# })
# df
}
source('~/GitHub/Webcrawler1/WebCrawler.R')
